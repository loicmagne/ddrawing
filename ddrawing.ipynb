{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies - Restart runtime after executing next cell\n"
      ],
      "metadata": {
        "id": "P3oRn6W0SjV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDVJ80xeQVMf",
        "outputId": "0f71483f-83df-4cd3-dd2d-350f371b3a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: svgpathtools in /usr/local/lib/python3.7/dist-packages (1.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from svgpathtools) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from svgpathtools) (1.4.1)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (from svgpathtools) (1.4.1)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from cssutils) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cssutils) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cssutils) (3.10.0.2)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.9)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.26)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "fatal: destination path 'diffvg' already exists and is not an empty directory.\n",
            "/content/diffvg\n",
            "WARNING:tensorflow:From setup.py:82: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "2022-01-19 21:42:19.036373: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing diffvg.egg-info/PKG-INFO\n",
            "writing dependency_links to diffvg.egg-info/dependency_links.txt\n",
            "writing requirements to diffvg.egg-info/requires.txt\n",
            "writing top-level names to diffvg.egg-info/top_level.txt\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'diffvg.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "running build_ext\n",
            "-- pybind11 v2.6.0 dev\n",
            "-- Using pybind11: (version \"2.6.0\" dev)\n",
            "-- Build without CUDA support\n",
            "INFO Building without TensorFlow support (not found)\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/diffvg/build/temp.linux-x86_64-3.7\n",
            "[100%] Built target diffvg\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/image.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/device.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/shape.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/__init__.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/pixel_filter.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/color.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/render_tensorflow.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "creating build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/save_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/image.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/device.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/parse_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/optimize_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/shape.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/render_pytorch.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/__init__.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/pixel_filter.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/color.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/diffvg.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/image.py to image.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/device.py to device.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/shape.py to shape.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/pixel_filter.py to pixel_filter.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/color.py to color.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/render_tensorflow.py to render_tensorflow.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/save_svg.py to save_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/image.py to image.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/device.py to device.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/parse_svg.py to parse_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/optimize_svg.py to optimize_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/shape.py to shape.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/render_pytorch.py to render_pytorch.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/pixel_filter.py to pixel_filter.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/color.py to color.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "creating 'dist/diffvg-0.0.1-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.7/dist-packages/diffvg-0.0.1-py3.7-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.7/dist-packages/diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "Extracting diffvg-0.0.1-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "diffvg 0.0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for diffvg==0.0.1\n",
            "Searching for svgpathtools==1.4.4\n",
            "Best match: svgpathtools 1.4.4\n",
            "Adding svgpathtools 1.4.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for svgwrite==1.4.1\n",
            "Best match: svgwrite 1.4.1\n",
            "Adding svgwrite 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for diffvg==0.0.1\n",
            "/content\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-kn62rii9\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-kn62rii9\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install svgwrite\n",
        "!pip install svgpathtools\n",
        "!pip install cssutils\n",
        "!pip install lpips\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install wandb\n",
        "!pip install timm\n",
        "\n",
        "# install diffvg\n",
        "!git clone https://github.com/BachiLi/diffvg\n",
        "%cd diffvg\n",
        "!git submodule update --init --recursive\n",
        "!python setup.py install\n",
        "%cd ..\n",
        "\n",
        "# install CLIP\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pydiffvg\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import random\n",
        "import lpips\n",
        "import clip\n",
        "import wandb\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from subprocess import call\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "jFmJXNMES55i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiable Drawing Wrapper"
      ],
      "metadata": {
        "id": "QRl30t_qSoVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DifferentiableDrawer():\n",
        "    def __init__(self, device, width, height, max_stroke_width=2., gamma=1.0, optim=torch.optim.Adam):\n",
        "        pydiffvg.set_print_timing(False)\n",
        "        pydiffvg.set_use_gpu(torch.cuda.is_available())\n",
        "        pydiffvg.set_device(device)\n",
        "        self.device = device\n",
        "        self.canvas_width, self.canvas_height = width, height\n",
        "        self.max_stroke_width, self.gamma = max_stroke_width, gamma\n",
        "        self.shapes = []\n",
        "        self.shape_groups = []\n",
        "        self.points_optim = optim([torch.tensor(0)], lr=1.)\n",
        "        self.color_optim = optim([torch.tensor(0)], lr=0.01)\n",
        "        self.width_optim = optim([torch.tensor(0)], lr=0.1)\n",
        "        self.renderer = pydiffvg.RenderFunction.apply\n",
        "\n",
        "    def add_shapes(self, n=256, shape=\"path\", pts_range=(1,4)):\n",
        "        \"\"\"\n",
        "        Add shapes to the set of shapes\n",
        "        The shape parameter should be :\n",
        "        - path\n",
        "        - filled_path \n",
        "        \"\"\"\n",
        "        # Define shapes\n",
        "        shapes = []\n",
        "        shape_groups = []\n",
        "        if shape==\"filled_path\":\n",
        "            for i in range(n):\n",
        "                num_segments = random.randint(pts_range[0], pts_range[1])\n",
        "                num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
        "                points = []\n",
        "                p0 = (random.random(), random.random())\n",
        "                points.append(p0)\n",
        "                for j in range(num_segments):\n",
        "                    radius = 0.05\n",
        "                    p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
        "                    p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
        "                    p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
        "                    points.append(p1)\n",
        "                    points.append(p2)\n",
        "                    if j < num_segments - 1:\n",
        "                        points.append(p3)\n",
        "                        p0 = p3\n",
        "                points = torch.tensor(points)\n",
        "                points[:, 0] *= canvas_width\n",
        "                points[:, 1] *= canvas_height\n",
        "                path = pydiffvg.Path(\n",
        "                    num_control_points = num_control_points,\n",
        "                    points = points,\n",
        "                    stroke_width = torch.tensor(1.0),\n",
        "                    is_closed = True\n",
        "                )\n",
        "                shapes.append(path)\n",
        "                path_group = pydiffvg.ShapeGroup(\n",
        "                    shape_ids = torch.tensor([len(shapes) - 1]),\n",
        "                    fill_color = torch.tensor([\n",
        "                        random.random(),\n",
        "                        random.random(),\n",
        "                        random.random(),\n",
        "                        random.random()\n",
        "                    ])\n",
        "                )\n",
        "                shape_groups.append(path_group)\n",
        "        elif shape==\"path\":\n",
        "            for i in range(n):\n",
        "                num_segments = random.randint(pts_range[0], pts_range[1])\n",
        "                num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
        "                points = []\n",
        "                p0 = (random.random(), random.random())\n",
        "                points.append(p0)\n",
        "                for j in range(num_segments):\n",
        "                    radius = 0.05\n",
        "                    p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
        "                    p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
        "                    p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
        "                    points.append(p1)\n",
        "                    points.append(p2)\n",
        "                    points.append(p3)\n",
        "                    p0 = p3\n",
        "                points = torch.tensor(points)\n",
        "                points[:, 0] *= canvas_width\n",
        "                points[:, 1] *= canvas_height\n",
        "                #points = torch.rand(3 * num_segments + 1, 2) * min(canvas_width, canvas_height)\n",
        "                path = pydiffvg.Path(\n",
        "                    num_control_points = num_control_points,\n",
        "                    points = points,\n",
        "                    stroke_width = torch.tensor(1.0),\n",
        "                    is_closed = False\n",
        "                )\n",
        "                shapes.append(path)\n",
        "                path_group = pydiffvg.ShapeGroup(\n",
        "                    shape_ids = torch.tensor([len(shapes) - 1]),\n",
        "                    fill_color = None,\n",
        "                    stroke_color = torch.tensor([\n",
        "                        random.random(),\n",
        "                        random.random(),\n",
        "                        random.random(),\n",
        "                        random.random()\n",
        "                    ])\n",
        "                )\n",
        "                shape_groups.append(path_group)\n",
        "        # Get parameters\n",
        "        points_vars = []\n",
        "        color_vars = []\n",
        "        stroke_width_vars = []\n",
        "        for path in shapes:\n",
        "            path.points.requires_grad = True\n",
        "            points_vars.append(path.points)\n",
        "        if shape==\"path\":\n",
        "            for path in shapes:\n",
        "                path.stroke_width.requires_grad = True\n",
        "                stroke_width_vars.append(path.stroke_width)\n",
        "        if shape==\"filled_path\":\n",
        "            for group in shape_groups:\n",
        "                group.fill_color.requires_grad = True\n",
        "                color_vars.append(group.fill_color)\n",
        "        else:\n",
        "            for group in shape_groups:\n",
        "                group.stroke_color.requires_grad = True\n",
        "                color_vars.append(group.stroke_color)\n",
        "        \n",
        "        # Add parameters to optimizer\n",
        "        points_new_optim = torch.optim.Adam(points_vars, lr=1.0)\n",
        "        color_new_optim = torch.optim.Adam(color_vars, lr=0.01)\n",
        "        width_new_optim = torch.optim.Adam(stroke_width_vars, lr=0.1)\n",
        "        self.points_optim.param_groups += points_new_optim.param_groups\n",
        "        self.color_optim.param_groups += color_new_optim.param_groups\n",
        "        self.width_optim.param_groups += width_new_optim.param_groups\n",
        "\n",
        "        self.shapes += shapes\n",
        "        self.shape_groups += shape_groups\n",
        "        \n",
        "    def zero_grad(self):\n",
        "        self.points_optim.zero_grad()\n",
        "        self.color_optim.zero_grad()\n",
        "        self.width_optim.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        self.points_optim.step()\n",
        "        self.color_optim.step()\n",
        "        self.width_optim.step()\n",
        "        try:\n",
        "            for group in self.shape_groups:\n",
        "                group.fill_color.data.clamp_(0.0, 1.0)\n",
        "        except AttributeError:\n",
        "            for path in self.shapes:\n",
        "                path.stroke_width.data.clamp_(1.0, self.max_stroke_width)\n",
        "            for group in self.shape_groups:\n",
        "                group.stroke_color.data.clamp_(0.0, 1.0)\n",
        "\n",
        "\n",
        "    def render(self, seed):\n",
        "        # Forward pass: render the image.\n",
        "        scene_args = pydiffvg.RenderFunction.serialize_scene(\n",
        "            self.canvas_width,\n",
        "            self.canvas_height,\n",
        "            self.shapes,\n",
        "            self.shape_groups\n",
        "        )\n",
        "        img = self.renderer(\n",
        "            self.canvas_width, # width\n",
        "            self.canvas_height, # height\n",
        "            2,   # num_samples_x\n",
        "            2,   # num_samples_y\n",
        "            seed,   # seed\n",
        "            None,\n",
        "            *scene_args\n",
        "        )\n",
        "        # Compose img with white background\n",
        "        img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = self.device) * (1 - img[:, :, 3:4])\n",
        "        # Save the intermediate render.\n",
        "        pydiffvg.imwrite(img.cpu(), 'results/painterly_rendering/iter_{}.png'.format(t), gamma=self.gamma)\n",
        "        img = img[:, :, :3]\n",
        "        # Convert img from HWC to NCHW\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
        "        return img"
      ],
      "metadata": {
        "id": "QPSfVHpkSs6v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer LPIPS"
      ],
      "metadata": {
        "id": "CEtwxbmGSlau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViT, self).__init__()\n",
        "        self.model = timm.create_model(\"vit_base_patch32_224\", pretrained=True)\n",
        "        self.model.eval()\n",
        "        self.model.head = torch.nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class FeatureSimilarityCriterion():\n",
        "    def __init__(self, device, net='vit'):\n",
        "        self.model = None\n",
        "        if net == 'vit':\n",
        "            self.model = ViT()\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
        "                                (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "        self.criterion = torch.nn.L1Loss()\n",
        "\n",
        "    def __call__(self,img,target):\n",
        "        img_batch = torch.cat([img,target])\n",
        "        features = self.model(self.transforms(img_batch))\n",
        "        loss = self.criterion(features[0],features[1])\n",
        "        return loss"
      ],
      "metadata": {
        "id": "BZXPztxUSktM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Optimization"
      ],
      "metadata": {
        "id": "PRdqWODxbhg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "540435e89dc8473b8666c0909d377f11",
            "44cd6c13995c415c8c4475303173da79",
            "2be7b21ad3bf45028cbf4a86787e88b6",
            "09de7db3504142e3a7b9a98a8af3c220",
            "51f5b89a8c28482f8eb4fef62d6555a3",
            "b92b1af8d39740558d5236d9c9dc1411",
            "b9e9b58167084c4f9e4894db716e7212",
            "6ded8ee1804f4cac8778c4f6d0d4c150",
            "acb3819f32ae480d92ff86383b4b96cd",
            "aabe28d18ea848948c7cb6282619a7b2",
            "0c0d416920a04ccc9c0ef91c26e6c13a"
          ]
        },
        "id": "JvqHg46_x8Wi",
        "outputId": "2817521d-8da7-4976-e3fe-641cfb41a0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "540435e89dc8473b8666c0909d377f11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/lmagne/recvis-project/runs/3o0uh4hv\" target=\"_blank\">L1</a></strong> to <a href=\"https://wandb.ai/lmagne/recvis-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [47:29<00:00,  5.70s/it, render_loss=0.0331]\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "n_paths = 256\n",
        "n_iter = 500\n",
        "max_stroke_width = 4\n",
        "gamma = 1.0\n",
        "percep_loss = False # Set True to compare image features instead of L2 loss\n",
        "use_filled_path = False # Set True to use filled curves instead of simple path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize target image\n",
        "target_img = np.array(Image.open(\"img1.png\").convert('RGB').resize((320,180))) # 320,180 300,200 \n",
        "target = torch.from_numpy(target_img).to(torch.float32) / 255.0\n",
        "target = target.pow(gamma)\n",
        "target = target.to(device)\n",
        "target = target.unsqueeze(0)\n",
        "target = target.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
        "canvas_width, canvas_height = target.shape[3], target.shape[2]\n",
        "\n",
        "# Initialize diffvg and drawer\n",
        "drawer = DifferentiableDrawer(\n",
        "    device, \n",
        "    width=canvas_width, \n",
        "    height=canvas_height, \n",
        "    max_stroke_width=max_stroke_width, \n",
        "    gamma=gamma, \n",
        "    optim=torch.optim.Adam\n",
        ")\n",
        "\n",
        "# Add paths\n",
        "drawer.add_shapes(n_paths, shape=\"path\", pts_range=(2,5))\n",
        "\n",
        "l2_criterion = torch.nn.L1Loss()\n",
        "percep_criterion = lpips.LPIPS(net='vgg')\n",
        "feat_criterion = FeatureSimilarityCriterion(device, net='clip')\n",
        "\n",
        "NAME = \"L1\"\n",
        "wandb.init(\n",
        "    project=\"recvis-project\",\n",
        "    name=NAME\n",
        ")\n",
        "\n",
        "with tqdm(range(n_iter)) as loop:\n",
        "    for t in loop:\n",
        "        metrics = {\n",
        "            'lpips': None,\n",
        "            'L2': None,\n",
        "            'feat': None,\n",
        "            'loss': None\n",
        "        }\n",
        "\n",
        "        drawer.zero_grad() # Zero grad\n",
        "        img = drawer.render(t) # Forward pass: render the image.\n",
        "\n",
        "        # Compute losses\n",
        "        l2_loss = l2_criterion(img, target)\n",
        "        lpips_loss = percep_criterion(img,target)\n",
        "        # feat_loss = feat_criterion(img,target)\n",
        "        loss =  l2_loss\n",
        "\n",
        "        loss.backward() # Backpropagate the gradients.\n",
        "        drawer.step() # Take a gradient descent step.\n",
        "\n",
        "        # Record metrics\n",
        "        with torch.no_grad():\n",
        "            metrics['lpips'] = lpips_loss.item()\n",
        "            # metrics['feat'] = feat_loss.item()\n",
        "            metrics['L2'] = l2_loss.item()\n",
        "            metrics['loss'] = loss.item()\n",
        "        loop.set_postfix(render_loss=loss.item())\n",
        "        wandb.log(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = torch.nn.MSELoss()\n",
        "print(l2(img, target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlQPkFyxfAbT",
        "outputId": "d8f4c470-7a14-49ea-b292-cf3a0cee443f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0043, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP-Guided Generation"
      ],
      "metadata": {
        "id": "2LOyfn5NbnCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "d95833cd1140447cb1d777dd0181e48c",
            "24a5fbad94c7453d86321dda94cd3a88",
            "004e77a061e14b18a1824ddf360df2ba",
            "30487cecd98c495aa698cca918f07969",
            "e2963ba5980749e98a65473b036ce5e1",
            "ef52b99a0cbf4e44b2cde1f3c657c20f",
            "f09ed97371744cbead04053f2320ce52",
            "835eac12131147b89536ebf8421f66c5"
          ]
        },
        "id": "R5WNy3CTWn8b",
        "outputId": "53302ef7-50f6-4f59-9247-d6a9f87169e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:nl4w3eg4) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 5722... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d95833cd1140447cb1d777dd0181e48c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▆▃▃▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>-1.0342</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">CLIP</strong>: <a href=\"https://wandb.ai/lmagne/recvis-project/runs/nl4w3eg4\" target=\"_blank\">https://wandb.ai/lmagne/recvis-project/runs/nl4w3eg4</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220120_014126-nl4w3eg4/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:nl4w3eg4). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/lmagne/recvis-project/runs/122iwhdu\" target=\"_blank\">CLIP</a></strong> to <a href=\"https://wandb.ai/lmagne/recvis-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 423/500 [37:07<10:25,  8.13s/it, render_loss=-1.81]"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "n_paths = 256\n",
        "n_iter = 500\n",
        "max_stroke_width = 50.\n",
        "gamma = 1.0\n",
        "n_augment = 4 # Number of augmentation per round\n",
        "use_filled_path = False # Set True to use filled curves instead of simple path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CLIP and target\n",
        "model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "model.eval()\n",
        "target_txt = clip.tokenize(\"Watercolor painting of an underwater submarine.\").to(device)\n",
        "with torch.no_grad():\n",
        "    target_features = model.encode_text(target_txt)\n",
        "canvas_width, canvas_height = 224, 224\n",
        "clip_transforms = transforms.Compose([\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
        "                         (0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
        "                         (0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "# Initialize diffvg and drawer\n",
        "drawer = DifferentiableDrawer(\n",
        "    device, \n",
        "    width=canvas_width, \n",
        "    height=canvas_height, \n",
        "    max_stroke_width=max_stroke_width, \n",
        "    gamma=gamma, \n",
        "    optim=torch.optim.Adam\n",
        ")\n",
        "\n",
        "# Add paths\n",
        "drawer.add_shapes(n_paths, shape=\"path\", pts_range=(1,3))\n",
        "\n",
        "l2_criterion = torch.nn.MSELoss()\n",
        "percep_criterion = lpips.LPIPS(net='vgg')\n",
        "\n",
        "NAME = \"CLIP\"\n",
        "wandb.init(\n",
        "    project=\"recvis-project\",\n",
        "    name=NAME\n",
        ")\n",
        "\n",
        "with tqdm(range(n_iter)) as loop:\n",
        "    for t in loop:\n",
        "        drawer.zero_grad() # Zero grad\n",
        "        img = drawer.render(t) # Forward pass: render the image.\n",
        "\n",
        "        # Compute losses\n",
        "        loss = 0\n",
        "        imgs = torch.cat([augmentation_transforms(img) for i in range(n_augment)])\n",
        "        imgs_features = model.encode_image(imgs)\n",
        "        for i in range(n_augment):\n",
        "            loss -= torch.cosine_similarity(target_features, imgs_features[i:i+1], dim=1)\n",
        "\n",
        "        loss.backward() # Backpropagate the gradients.\n",
        "        drawer.step() # Take a gradient descent step.\n",
        "\n",
        "        loop.set_postfix(render_loss=loss.item())\n",
        "        wandb.log({'loss': loss.item()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "Nf1vwLhJwgN3",
        "outputId": "6b0c36ab-cef8-4444-f516-c460be2a7a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from 'results/painterly_rendering/iter_%d.png':\n",
            "  Duration: 00:00:08.33, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 320x180, 60 fps, 60 tbr, 60 tbn, 60 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mprofile High 4:4:4 Predictive, level 3.2, 4:4:4 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=abr mbtree=1 bitrate=40000 ratetol=1.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'results/painterly_rendering/out.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv444p, 320x180, q=-1--1, 40000 kb/s, 60 fps, 15360 tbn, 60 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/40000000 buffer size: 0 vbv_delay: -1\n",
            "frame=  500 fps= 46 q=-1.0 Lsize=   26555kB time=00:00:08.28 bitrate=26261.6kbits/s speed=0.757x    \n",
            "video:26548kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.025875%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mframe I:2     Avg QP: 1.12  size: 60238\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mframe P:134   Avg QP: 0.68  size: 66697\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mframe B:364   Avg QP: 2.59  size: 49798\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mconsecutive B-frames:  2.8%  0.4%  0.0% 96.8%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mmb I  I16..4:  9.4%  0.0% 90.6%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mmb P  I16..4:  2.6%  0.0% 21.2%  P16..4: 11.8% 28.8% 35.6%  0.0%  0.0%    skip: 0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mmb B  I16..4:  1.0%  0.0%  6.5%  B16..8: 15.8% 15.7% 31.5%  direct:29.5%  skip: 0.0%  L0:32.8% L1:21.6% BI:45.7%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mfinal ratefactor: -11.84\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mcoded y,u,v intra: 80.8% 77.4% 77.9% inter: 96.6% 95.9% 96.3%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mi16 v,h,dc,p: 28% 36% 30%  6%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 23% 16%  5%  6%  4%  6%  3%  6%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mWeighted P-Frames: Y:16.4% UV:3.0%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mref P L0: 37.7% 18.0% 22.0% 21.6%  0.6%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mref B L0: 76.9% 17.0%  6.1%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mref B L1: 94.0%  6.0%\n",
            "\u001b[1;36m[libx264 @ 0x55653c215e00] \u001b[0mkb/s:26096.84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4f88882d-3b02-456b-9731-086c15704dbc\", \"out.mp4\", 27191947)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "!ffmpeg -y -framerate 60 -i results/painterly_rendering/iter_%d.png -vb 40M results/painterly_rendering/out.mp4\n",
        "files.download('results/painterly_rendering/out.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r results/painterly_rendering/"
      ],
      "metadata": {
        "id": "l7zyIPphTxFm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZzFnKt2EluBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "recvis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "540435e89dc8473b8666c0909d377f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_44cd6c13995c415c8c4475303173da79",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2be7b21ad3bf45028cbf4a86787e88b6",
              "IPY_MODEL_09de7db3504142e3a7b9a98a8af3c220",
              "IPY_MODEL_51f5b89a8c28482f8eb4fef62d6555a3"
            ]
          }
        },
        "44cd6c13995c415c8c4475303173da79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2be7b21ad3bf45028cbf4a86787e88b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b92b1af8d39740558d5236d9c9dc1411",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9e9b58167084c4f9e4894db716e7212"
          }
        },
        "09de7db3504142e3a7b9a98a8af3c220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ded8ee1804f4cac8778c4f6d0d4c150",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acb3819f32ae480d92ff86383b4b96cd"
          }
        },
        "51f5b89a8c28482f8eb4fef62d6555a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aabe28d18ea848948c7cb6282619a7b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:04&lt;00:00, 130MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c0d416920a04ccc9c0ef91c26e6c13a"
          }
        },
        "b92b1af8d39740558d5236d9c9dc1411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9e9b58167084c4f9e4894db716e7212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ded8ee1804f4cac8778c4f6d0d4c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acb3819f32ae480d92ff86383b4b96cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aabe28d18ea848948c7cb6282619a7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c0d416920a04ccc9c0ef91c26e6c13a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d95833cd1140447cb1d777dd0181e48c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24a5fbad94c7453d86321dda94cd3a88",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_004e77a061e14b18a1824ddf360df2ba",
              "IPY_MODEL_30487cecd98c495aa698cca918f07969"
            ]
          }
        },
        "24a5fbad94c7453d86321dda94cd3a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "004e77a061e14b18a1824ddf360df2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_e2963ba5980749e98a65473b036ce5e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef52b99a0cbf4e44b2cde1f3c657c20f"
          }
        },
        "30487cecd98c495aa698cca918f07969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f09ed97371744cbead04053f2320ce52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_835eac12131147b89536ebf8421f66c5"
          }
        },
        "e2963ba5980749e98a65473b036ce5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef52b99a0cbf4e44b2cde1f3c657c20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f09ed97371744cbead04053f2320ce52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "835eac12131147b89536ebf8421f66c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}